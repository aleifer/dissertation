%!TEX root = /Users/andy/Documents/Academics/Dissertation/thesis.tex
%
% https://github.com/downloads/aleifer/dissertation/thesis.pdf

\begin{savequote}[75mm] 
This is some random quote to start off the chapter.
\qauthor{Firstname lastname} 
\end{savequote}

\chapter{Maximum Likelihood Estimator Bayesian Inference DNA Origami  Nanorod Barcode Detection}

\section{Introduction}
\newthought{The development of next generation microarray and on-chip technologies} will depend in part upon advancements in fluorescent reporter molecules and improvements in their detection.

Currently accepted microarray techniques  use at most two spectrally distinct fluorophores to identify their targets.  Recently, however, there has been an explosion in fluorescent reporter approaches that enables an observer to uniquely identify one out of many hundreds of distinct fluorescent encoded reporters. Researchers have explored both combining spectrally distinct fluorophores in close proximity to create gradations of colors[cite a bunch of stuff], and spatially separating fluorophores to create geometric optical barcodes [cite a bunch of stuff]. In particular, fluorescently encoded DNA based nanorods [cite the DNA related ones] provide a promising avenue to DNA based micro-array techniques. Instead of identifying DNA microarray targets largely by location, this raises the distinct possibility of also unambiguously identifying targets by fluorescent encoding. 

Programmatically identifying images of these fluorescently encoded barcodes, however, presents unique computational challenges that have not yet been explored in the literature.  Here we describe a mathematical formalism that allows us to use computer vision to optimally detect DNA origami-based nanorod barcodes. We take a bayesian inference approach, and demonstrate that our implementation allows for >blah \% correct barcode identification under real-world situations. 

\section{System and Methods}

\subsection{DNA origami nanorod barcodes}
We chose to image DNA origami nanorod barcodes [cite chenxiang]. These nanorods consist of mechanically rigid six-helix bundles of DNA with radius 5 nm and length XY \textmu m decorated with  up to two spectrally distinct fluorophores at three distinct locations along the rod, as illustrated in Fig.~\ref{nanoAnalysis}. The three fluorescing locations are unevenly spaced, such that there is a larger gap of \textasciitilde 450 \textmu m and a smaller gap of \textasciitilde 270 \textmu m  between the fluorophores on each rod, giving the barcodes a visible anisotropy. When using combinations of red, green and blue fluorophores, (Cy5, Cy3 and Alexa Fluor 488, respectively) this allows for a dictionary of six fluorescing symbols at each of the three barcode locations: \{Red, Green, Blue, RedGreen, RedBlue, GreenBlue\}, for a total of $6^{3}=216$  unambiguous species of barcode.

We chose to work with this particular form of fluorescently encoded barcodes as opposed to others because this form offer the largest published number of distinct barcode species to date, has the advantage of being massively self-assembled in parallel, and creates barcodes of suitably small  size so as to be useful for future microarray applications. 

The barcodes are, in fact, so small that the distance between fluorescing sites is often smaller than a wavelength of light. As a result, when the barcodes are imaged on a flat surface, the distinct flourescent sites appear to blend into one another. This becomes one of the primary challenges facing any computer vision barcode identification software, namely to  decode the barcode even when the individual difraction-limited fluorescent spots overlap. 

\subsection{Imaging}
Barcodes were imaged on a glass cover slide on a BLAH BLAH microscope,  under total internal reflection fluorescence (TIRF). 
The red, green and blue channel images of the barcodes were taken on a glass coverslip, with purple (XXXnm), green (XXXnm) or blue (XXXnm) laser light at YYY mW, ZZZ mW and XXXX mW incident power, respectively. A BLAH BLAH EMCCD camera was used with an exposure time of BLAH ms.
 
\subsection{Task at Hand}
To be useful for microarray or other diagnostic applications, it is important for any computer vision software to be able to automatically detect and decode large quantities of barcodes robustly and accurately with little or no user input. 
Here we restrict ourselves to immobilized barcodes laying flat on a glass slide, but we demand that our software solution tolerate  likely real-world complexities including the presence of inhomogeneous spatial illumination, background fluorescence and minor defects in barcode folding.  
In the following sections we develop an optimal mathematical framework and software implementation for identifying the location and orientation of the barcode and then decoding it.


\section{Theory and Algorithm}
To automatically detect and decode a barcode from its three-channel image, the software must systematically scan through the image, detect the location of each barcode and decide which of the 216 reference barcode species best fits the observed barcode. Assigning the best fitting reference, or model, barcode is performed by Bayesian multiple hypothesis testing. This is, in general, computational intensive for two-dimensional datasets, so we first turn for guidance to a one-dimensional problem.


\subsection{One dimension, single-channel}
In the one dimensional problem we assume we have $K$ possible one-dimensional reference signals in a system with zero-mean additive Gaussian noise $n$, such that for any reference $m$, the observed signal $\mathbf{u}$ is, in vector notation,
\begin{equation}
\mathbf{u}=\mathbf{m} + \mathbf{n}.
\end{equation}
Given a noisy observation $\mathbf{u}$, we seek the most likely corresponding reference. 

%%%
% Suggestions for Notation
% Added Monday: since we'll talk about 1D case first, let's use 'u' for 1D. 'v' is then natural for 2D data that follows.
% j=1...J, where J=3. Color index (red green blue).
% \mathbf{m}_k = kth model, K = total # barcodes
% v(x,y) = raw observed voltage at pixel x,y. Probably won't mention the x,y coordinates for most of what we do
% c = camera offset constant
% n(x,y) = noise
% so v = a*\mathbf{m}_k + n + c where a is ampl
% u(x) = 1D case, which is what reduced observed data (offset removed, scale flattened, and projected) becomes
% std probability notation p(H|D) = prob. of H given D
% \mathbf{R}_n = noise covariance matrix
% \sigma_j^2 = noise variance of jth color image
% <.> = expectation (or use overbar)


The probablity that the observed signal $\mathbf{u}$ is composed of the $k$th reference signal $\mathbf{m}_k$ is given by Bayes' theorem,
\begin{equation}\label{eq:Bayes}
p(\mathbf{m}_k|\mathbf{u}) = \frac{p(\mathbf{u}|\mathbf{m}_k)p(\mathbf{m}_k)} {p(\mathbf{u})}.
\end{equation}
where $p(\mathbf{m}_k|\mathbf{u})$ is the inverse probability, $p(\mathbf{u}|\mathbf{m}_k)$ is the forward probability or likelihood of seeing data $\mathbf{u}$ given the presence of model $\mathbf{m}_k$, $p(\mathbf{m}_k)$ is the prior probability that model $\mathbf{m}_k$ is present, and $p(\mathbf{u})$ is a normalization constant.

We seek to compare the relative  probability for each possible reference, $\mathbf{m}_k$ for $k=1,2,3 \ldots K$ and select the most likely.  

While testing a given observed signal $\mathbf{u}$ against all models, the denominator is constant and independent of $k$. Maximizing $p(\mathbf{m}_k|\mathbf{u})$ is therefore equivalent to finding the model $k$ that maximizes the product  $p(\mathbf{u}|\mathbf{m}_k)p(\mathbf{m}_k)$.
In the present case where all barcodes are equally probable so that
$p(\mathbf{m}_k)=1/K,$ 
the search simplifies to finding the maximum likelihood 
\begin{equation}
\Lambda_k = p(\mathbf{u}|\mathbf{m}_k).
\end{equation}



The probability $p(\mathbf{u}|\mathbf{m}_k)$ of observing $\mathbf{u}$ given the reference $\mathbf{m}_k$  is simply the probability that the difference $\mathbf{u}-\mathbf{m}_k$ between the signal and the reference is generated by noise.
\begin{equation}\label{eq:Main}
\Lambda_k = p(\mathbf{u}|\mathbf{m}_k) = \frac{1}{  \sqrt{ (2\pi)^N \det || \mathbf{R}_n||} } \exp\left[ -\frac{1}{2}  (\mathbf{u}-\mathbf{m}_k)^T \mathbf{R}_n^{-1} (\mathbf{u}-\mathbf{m}_k) \right]
\end{equation}
where $N$ is the length of vector $\mathbf{u}$ and $\mathbf{R}_n$ is the noise covariance matrix. This is the multivariate probability density function for a random Gaussian variable,  $(\mathbf{u}-\mathbf{m}_k)$ [REFERENCE].

For the case of identical independently distributed (IID) Gaussian noise, 
\begin{equation}
\mathbf{R}_n=\sigma^2 \mathbf{I}
\end{equation}
where $\mathbf{I}$ is the identity matrix and $\sigma^2$ is the variance of the noise. The probability thus reduces further to
\begin{equation}\label{eq:iidEnergy}
\Lambda_k = p(\mathbf{u}|\mathbf{m}_k) = \frac{1}{  \sigma^N \sqrt{ (2\pi)^N}   } \exp\left[ -\frac{(\mathbf{u}-\mathbf{m}_k)^T(\mathbf{u}-\mathbf{m}_k)} {2 N \sigma^2 } \right].
\end{equation}

The exponent is simply the energy contained in the sequence $\mathbf{u}-\mathbf{m}_k$ divided by the
theoretical energy that a sequence of length $N$ would have were it pure noise with variance $\sigma^2$. We recognize parallels to the Maxwell-Boltzmann distribution of statistical
mechnanics [CITE REIF].  

We find the $\mathbf{m}_k$ that maximizes equation \ref{eq:iidEnergy},
\begin{equation}
\max_k   \left\{ p(\mathbf{u}|\mathbf{m}_k) \right\} =  \max_k  \left\{ \frac{1}{  \sqrt{ (2\pi)^N} \sigma^N  } \exp\left[ -\frac{(\mathbf{u}-\mathbf{m}_k)^T(\mathbf{u}-\mathbf{m}_k)} {2 N \sigma^2 } \right] \right\}. 
\end{equation}
We note that $\sigma$ and $N$ are independent of $k$, and that the exponential is a monotonically increasing function, thus we must only minimize across $k$ the energy of the difference between the observation and the reference,

\begin{equation}
\max_k   \left\{ p(\mathbf{m}_k|\mathbf{u}) \right\} =  \min_k  \left\{ (\mathbf{u}-\mathbf{m}_k)^T(\mathbf{u}-\mathbf{m}_k) \right\}. 
\end{equation}

We rewrite this in terms of a sum,
\begin{equation}
\max_k   \left\{ p(\mathbf{m}_k|\mathbf{u}) \right\} =  \min_k  \left\{ \sum_{i=1}^N  (u_i-m_{k,i})^2   \right\}. 
\end{equation}
It is noteworthy that the Bayesian hypothesis test is equivalent in this case to minimizing the mean squared error between observation and reference.


\subsection{One dimension, multi-channel}\label{sec:simpleModel}
We now consider a system where each reference $\mathbf{m}_k$ consists of three one-dimensional vectors of length $N$, $\mathbf{m}_{k1}$, $\mathbf{m}_{k2}$ and $\mathbf{m}_{k3}$ each in its own channel. Consequently, the signal $\mathbf{u}$ now consists of three one dimensional observation $\mathbf{u}_1$, $\mathbf{u}_2$ and $\mathbf{u}_3$, in each channel. As before, we assume that there are $K$ references and that each channel has zero-mean additive IID Gaussian noise, with variances $\sigma_1^2$, $\sigma_2^2$ and $\sigma_3^2$ respectively. As we will see later, these channels can be thought of as color channels in a one-dimensional composite red, green and blue image. 

Because the three channels are independent,  the likelihood that  signal $\mathbf{u}$ is observed when $\mathbf{m}_k$ is present is simply the likelihood that  $\mathbf{u}_1$ is observed when $\mathbf{m}_{k1}$ is present times the likelihood that  $\mathbf{u}_2$ is observed when $\mathbf{m}_{k2}$ is present times the likelihood that  $\mathbf{u}_3$ is observed when $\mathbf{m}_{k3}$ is present [CITE ELEMENTARY PROBABILITY],
\begin{equation}
p(\mathbf{u}|\mathbf{m}_k) = p(\mathbf{u}_1|\mathbf{m}_{k1})p(\mathbf{u}_2|\mathbf{m}_{k2})p(\mathbf{u}_3|\mathbf{m}_{k3}).
\end{equation}

Substituting in from Eq.~\ref{eq:iidEnergy},
\begin{multline}
\Lambda_k=p(\mathbf{u}|\mathbf{m}_k) =\frac{1}{ (\sigma_1\sigma_2\sigma_3)^N  \sqrt{ (2\pi)^{3N}}  }  \exp\Bigg[ \frac{-(\mathbf{u}-\mathbf{m}_{k1})^T(\mathbf{u}-\mathbf{m}_{k1})} {2 N \sigma_1^2 }   + \\ \frac{-(\mathbf{u}-\mathbf{m}_{k2})^T(\mathbf{u}-\mathbf{m}_{k2})} {2 N \sigma_2^2 } +\frac{-(\mathbf{u}-\mathbf{m}_{k3})^T(\mathbf{u}-\mathbf{m}_{k3})} {2 N \sigma_3^2 }    \Bigg].
\end{multline}

As before, we can factor out and ignore terms that have no $k$ dependence. Thus the maximum likelihood model $k$ correspond to,
\begin{multline}
\max_k   \big\{ p(\mathbf{m}_k|\mathbf{u}) \big\} =  \min_k  \Bigg\{  \frac{(u_1-\mathbf{m}_{k1})^T(u_1-\mathbf{m}_{k1})}{\sigma_1^2} +\\  
\frac{(u_2-\mathbf{m}_{k2})^T(u_2-\mathbf{m}_{k2})}{\sigma_2^2} + \frac{(u_3-\mathbf{m}_{k3})^T(u_3-\mathbf{m}_{k3})}{\sigma_3^2} \Bigg\}. 
\end{multline}




\subsection{2D Multi-Channel Image}
We would like to use the simple model described above to identify the barcodes in our two-dimensional images. Before we can do so, we must reconcile the following discrepencies between assumptions in the simple model, and the realities of our barcode dataset:
	\begin{enumerate} %Why is this datest different from all other datasets?
	\item The simple model assumes that the noise is Gaussian, additive, zero-mean, and IID. But our images exhibit a slowly varying offset, and therefore exhibit noise that is neither zero-mean nor IID. \label{item:noise}
	\item The simple model assumes that the observation $\mathbf{u}$ contains exactly one reference at a known location, but our images contain an unknown number of barcodes at unknown locations. 
	\item The simple model assumes that the reference $\mathbf{m_k}$ and observation $\mathbf{u}$ waveforms have the same known height, but in our images the height is \textit{a priori} unknown and may vary within an image.  
	\item The simple model assumes a one-dimensional image, but our images are two dimensional.
	\end{enumerate} 
We will address  the discrepancies listed above by employing image processing techniques to transform our raw images into a dataset that can be analyzed using the simple one-dimensional multi-channel model described in Section \ref{sec:simpleModel}. 

To reconcile item (\ref{item:noise}), we need to fully account for the various contributions to the signal recorded at our detector. The voltage $V(\mathbf{r})$ at a pixel $\mathbf{r}$ of our detector is the sum of a term that is proportional to the number of inicident photons, plus a DC offset $V_0(\mathbf{r})$ that can slowly vary from pixel to pixel, plus thermal noise $n_{\text{therm}}$,
\begin{equation}
V(\mathbf{r})=V_0(\mathbf{r})+c \cdot \text{Photons}(\mathbf{r}) +n_{\text{therm}}.
\end{equation}
Recall that in our system, laser light is incident on the sample, but only photons that have been re-emitted at a particular wavelength are capable of reaching our detector.  Photons that reach the detector must have been re-emitted from either the fluorophores on the barcodes themselves $f$ or from the background media $b$, which consists of a glass slide and buffer.  In both cases, we model the photons that are re-emitted as proportional to the number of incident photons plus some noise term,
\begin{eqnarray}
f(\mathbf{r})&=&\gamma_{\text{ex}}(\mathbf{r}) ( q_{\text{fluor}}(\mathbf{r}) + n_{\text{fluor}} )\\
b(\mathbf{r})&=&\gamma_{\text{ex}}(\mathbf{r}) ( q_{\text{back}} + n_{\text{back}} ).
\end{eqnarray}
Where  $\gamma_{\text{ex}}(\mathbf{r})$ is the slowly varying position-dependent intensity of the excitation laser,  $q_{\text{fluor}}(\mathbf{r})$ describes the position-dependent fluorescent efficiencty of the barcodes independent of illumination, and $q_{\text{background}}$ is the background fluorescent efficiency of the media which we assume is independent of position and illumination. 

Thus by recoding images when barcodes or the media or both are present, we can record the  images with our detector,
\begin{eqnarray}
V_{\text{fluor}}(\mathbf{r})&=&V_0(\mathbf{r})+c \cdot \big( f(\mathbf{r}) + b(\mathbf{r})\big)  +n_{\text{therm}}\\
V_{\text{back}}(\mathbf{r})&=&V_0(\mathbf{r})+c \cdot  b(\mathbf{r})  +n_{\text{therm}}\\
V_{\text{empty}}(\mathbf{r})&=&V_0(\mathbf{r})+n_{\text{therm}}
\end{eqnarray}
which contain barcodes+media, media only, or neither barcodes nor media, respectively. See Fig BLAH BLAH. 

We seek to construct an image that contains our underlying signal $q_{\text{fluor}}(\mathbf{r})$ and has zero-mean, IID noise. We subtract off the smoothed background and divide off spatially varying illumination,
\begin{equation}
V_{\text{processed}}(\mathbf{r})= \frac{ V_{\text{fluor}}(\mathbf{r}) - \langle V_{\text{back}}(\mathbf{r}) \rangle }{ \langle V_{\text{back}}(\mathbf{r}) \rangle - \langle V_{\text{empty}}(\mathbf{r}) \rangle}
\end{equation}
where the expectation value here denotes local smoothing in the image. Substituting,
\begin{equation}
V_{\text{processed}}(\mathbf{r})= \frac{ c \cdot \big( f(\mathbf{r})  +b(\mathbf{r}) -\langle b(\mathbf{r}) \rangle \big)+ n_{\text{therm}} - \langle n_{\text{therm}}  \rangle }{   c \cdot \langle b(\mathbf{r}) \rangle }
\end{equation}
Note that we assume $\langle V_0(\mathbf{r}) \rangle = V_0(\mathbf{r})$ and $\langle \gamma_{\text{ex}}(\mathbf{r}) \rangle  = \gamma_{\text{ex}}(\mathbf{r})$ because both processes are slowly spatially varying. Substituting further, 
\begin{equation}
V_{\text{processed}}(\mathbf{r})= \frac{ c \cdot \big(       \gamma_{\text{ex}}(\mathbf{r}) ( q_{\text{fluor}}(\mathbf{r}) + n_{\text{fluor}} )      +   n_{\text{back}} - \langle n_{\text{back}}  \rangle  \big) + n_{\text{therm}} - \langle n_{\text{therm}}  \rangle }{     c \big( \gamma_{\text{ex}}(\mathbf{r}) ( q_{\text{back}} +  \langle n_{\text{back}} \rangle) \big) }
\end{equation}
We say that the smoothed noise terms for thermal noise and background are zero, because any noise offset has been absorbed by their respective dc offset terms.  [Maybe I need to relabel those guys as delta n ?]



\subsection{Garbage	} 

We also assume that the nature of fluorescence itself is noisy, and since we assume the integration time of our detector is long, and thus each pixel will contain many many photons, we assume that the by the central limit theorem the noise from fluorescence should be Gaussian.






Before we can use the formalisms described above, we must first ensure that our data matches the assumptions: gaussian, IID, mean zero, one-dimensional. 

Without processing, the noise in the two-dimension, three-channel nanorod barcode images are non-gaussian, non-identicially distributed, they lack a zero offset and they are two dimensional.



Now we seek to reduce our three-channel two-dimensional barcode images into three-channel one-dimensional data. 



%bold non-ital uppercase for matrices
%bold non-ital lowercase for vectors


%Walk through



ASSUMPTION: The camera is just a photon counter. The noise we see comes overwhelmingly from "background fluorescence". We assume it is additive, thus we assume that our signal is merely superposed on top of the mean background. 

This implies that there is a single true peak height that is valid for all peaks. To test this assumption, I should background subtract the local mean from the barcodes and measure the height single isolated peaks. The height of locally background subtracted single isolated peaks should be flat and not correlate with location in the image.

This would mean that when identifying our barcode we don't have to scale our reference, we can use a single reference for all barcodes regardless of the location.


\section{Manuscript Information}
\subsection{Submitted for Publication As}
A version of this chapter has been submitted for publication in the journal \textit{Bioinformatics}.
% in \citep{leifer_optogenetic_2011}:
%\bibentry{leifer_optogenetic_2011}

\subsection{The Author's Contribution}
Andrew M.~Leifer wrote all of the software, generated all figures, developed portions of the mathematical framework, and wrote the majority of the manuscript. Mark C.~Leifer developed portions of the mathematical framework and wrote selected passages of the manuscript. Chenxiang Lin created the DNA origami barcodes and conducted all microscopy work. 
 